{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ---\
title: Conversational AI overview\
headline: Introduction - Conversational voice AI agents\
subtitle: 'Deploy customized, conversational voice agents in minutes.'\
---\
\
<div style=\{\{ position: 'relative', width: '100%', paddingBottom: '56.25%' \}\}>\
  <iframe\
    src="https://player.vimeo.com/video/1029660636"\
    frameBorder="0"\
    style=\{\{ position: 'absolute', top: '0', left: '0', width: '100%', height: '100%' \}\}\
    className="aspect-video w-full rounded-lg"\
    allow="autoplay; fullscreen; picture-in-picture"\
    allowFullScreen\
  />\
</div>\
\
## What is Conversational AI?\
\
ElevenLabs [Conversational AI](https://elevenlabs.io/conversational-ai) is a platform for deploying customized, conversational voice agents. Built in response to our customers' needs, our platform eliminates months of development time typically spent building conversation stacks from scratch. It combines these building blocks:\
\
<CardGroup cols=\{2\}>\
  <Card title="Speech to text">\
    Our fine tuned ASR model that transcribes the caller's dialogue.\
  </Card>\
  <Card title="Language model">\
    Choose from Gemini, Claude, OpenAI and more, or bring your own.\
  </Card>\
  <Card title="Text to speech">\
    Our low latency, human-like TTS across 5k+ voices and 31 languages.\
  </Card>\
  <Card title="Turn taking model">\
    Our custom turn taking model that understands when to speak, like a human would.\
  </Card>\
</CardGroup>\
\
Altogether it is a highly composable AI Voice agent solution that can scale to thousands of calls per day. With [server](/docs/conversational-ai/customization/tools/server-tools) & [client side](/docs/conversational-ai/customization/tools/client-tools) tools, [knowledge](/docs/conversational-ai/customization/knowledge-base) bases, [dynamic](/docs/conversational-ai/customization/personalization/dynamic-variables) agent instantiation and [overrides](/docs/conversational-ai/customization/personalization/overrides), plus built-in monitoring, it's the complete developer toolkit.\
\
<Card title="Pricing" horizontal>\
  15 minutes to get started on the free plan. Get 13,750 minutes included on the Business plan at\
  \\$0.08 per minute on the Business plan, with extra minutes billed at \\$0.08, as well as\
  significantly discounted pricing at higher volumes.\
  <br />\
  **Setup & Prompt Testing**: billed at half the cost.\
</Card>\
\
<Note>\
  Usage is billed to the account that created the agent. If authentication is not enabled, anybody\
  with your agent's id can connect to it and consume your credits. To protect against this, either\
  enable authentication for your agent or handle the agent id as a secret.\
</Note>\
\
## Pricing tiers\
\
<Tabs>\
  <Tab title="In Minutes">\
  \
  | Tier     | Price   | Minutes included | Cost per extra minute              |\
  | -------- | ------- | ---------------- | ---------------------------------- |\
  | Free     | \\$0     | 15               | Unavailable                        |\
  | Starter  | \\$5     | 50               | Unavailable                        |\
  | Creator  | \\$22    | 250              | ~\\$0.12                            |\
  | Pro      | \\$99    | 1100             | ~\\$0.11                            |\
  | Scale    | \\$330   | 3,600            | ~\\$0.10                            |\
  | Business | \\$1,320 | 13,750           | \\$0.08 (annual), \\$0.096 (monthly) |\
\
  </Tab>\
  <Tab title="In Credits">\
  \
  | Tier     | Price   | Credits included | Cost in credits per extra minute |\
  | -------- | ------- | ---------------- | -------------------------------- |\
  | Free     | \\$0     | 10,000           | Unavailable                      |\
  | Starter  | \\$5     | 30,000           | Unavailable                      |\
  | Creator  | \\$22    | 100,000          | 400                              |\
  | Pro      | \\$99    | 500,000          | 454                              |\
  | Scale    | \\$330   | 2,000,000        | 555                              |\
  | Business | \\$1,320 | 11,000,000       | 800                              |\
\
  </Tab>\
</Tabs>\
\
<Note>\
  Today we're covering the LLM costs, though these will be passed through to customers in the\
  future.\
</Note>\
\
### Pricing during silent periods\
\
When a conversation is silent for longer than ten seconds, ElevenLabs reduces the inference of the turn-taking model and speech-to-text services until voice activity is detected again. This optimization means that extended periods of silence are charged at 5% of the usual per-minute cost.\
\
This reduction in cost:\
\
- Only applies to the period of silence.\
- Does not apply after voice activity is detected again.\
- Can be triggered at multiple times in the same conversation.\
\
## Models\
\
Currently, the following models are natively supported and can be configured via the agent settings:\
\
- Gemini 2.0 Flash\
- Gemini 1.5 Flash\
- Gemini 1.5 Pro\
- Gemini 1.0 Pro\
- GPT-4o Mini\
- GPT-4o\
- GPT-4 Turbo\
- GPT-3.5 Turbo\
- Claude 3.5 Sonnet\
- Claude 3 Haiku\
\
![Supported models](file:72157348-cbbf-47bb-b400-d854491cfe43)\
\
You can start with our [free tier](https://elevenlabs.io/app/sign-up), which includes 15 minutes of conversation per month.\
\
Need more? Upgrade to a [paid plan](https://elevenlabs.io/pricing/api) instantly - no sales calls required. For enterprise usage (6+ hours of daily conversation), [contact our sales team](https://elevenlabs.io/contact-sales) for custom pricing tailored to your needs.\
\
## Popular applications\
\
Companies and creators use our Conversational AI orchestration platform to create:\
\
- **Customer service**: Assistants trained on company documentation that can handle customer queries, troubleshoot issues, and provide 24/7 support in multiple languages.\
- **Virtual assistants**: Assistants trained to manage scheduling, set reminders, look up information, and help users stay organized throughout their day.\
- **Retail support**: Assistants that help customers find products, provide personalized recommendations, track orders, and answer product-specific questions.\
- **Personalized learning**: Assistants that help students learn new topics & enhance reading comprehension by speaking with books and [articles](https://elevenlabs.io/blog/time-brings-conversational-ai-to-journalism).\
\
<Note>\
  Ready to get started? Check out our [quickstart guide](/docs/conversational-ai/quickstart) to\
  create your first AI agent in minutes.\
</Note>\
\
## FAQ\
\
<AccordionGroup>\
  <Accordion title="Concurrency limits">\
Plan limits\
\
Your subscription plan determines how many calls can be made simultaneously.\
\
| Plan       | Concurrency limit |\
| ---------- | ----------------- |\
| Free       | 4                 |\
| Starter    | 6                 |\
| Creator    | 10                |\
| Pro        | 20                |\
| Scale      | 30                |\
| Business   | 30                |\
| Enterprise | Elevated          |\
\
    <Note>\
      To increase your concurrency limit [upgrade your subscription plan](https://elevenlabs.io/pricing/api)\
      or [contact sales](https://elevenlabs.io/contact-sales) to discuss enterprise plans.\
    </Note>\
\
  </Accordion>\
  <Accordion title="Supported audio formats">\
    The following audio output formats are supported in the Conversational AI platform:\
\
    - PCM (8 kHz / 16 kHz / 22.05 kHz / 24 kHz / 44.1 kHz)\
    - \uc0\u956 -law 8000Hz\
\
  </Accordion>\
</AccordionGroup>\
\
\
---\
title: Events\
subtitle: >-\
  Understand real-time communication events exchanged between client and server\
  in conversational AI.\
---\
\
## Overview\
\
Events are the foundation of real-time communication in conversational AI applications using WebSockets.\
They facilitate the exchange of information like audio streams, transcriptions, agent responses, and contextual updates between the client application and the server infrastructure.\
\
Understanding these events is crucial for building responsive and interactive conversational experiences.\
\
Events are broken down into two categories:\
\
<CardGroup cols=\{2\}>\
  <Card\
    title="Client Events (Server-to-Client)"\
    href="/conversational-ai/customization/events/client-events"\
    icon="cloud-arrow-down"\
  >\
    Events sent from the server to the client, delivering audio, transcripts, agent messages, and\
    system signals.\
  </Card>\
  <Card\
    title="Client-to-Server Events"\
    href="/conversational-ai/customization/events/client-to-server-events"\
    icon="cloud-arrow-up"\
  >\
    Events sent from the client to the server, providing contextual updates or responding to server\
    requests.\
  </Card>\
</CardGroup>\
---\
title: Client events\
subtitle: >-\
  Understand and handle real-time events received by the client during\
  conversational applications.\
---\
\
**Client events** are system-level events sent from the server to the client that facilitate real-time communication. These events deliver audio, transcription, agent responses, and other critical information to the client application.\
\
<Note>\
  For information on events you can send from the client to the server, see the [Client-to-server\
  events](/docs/conversational-ai/customization/events/client-to-server-events) documentation.\
</Note>\
\
## Overview\
\
Client events are essential for maintaining the real-time nature of conversations. They provide everything from initialization metadata to processed audio and agent responses.\
\
<Info>\
  These events are part of the WebSocket communication protocol and are automatically handled by our\
  SDKs. Understanding them is crucial for advanced implementations and debugging.\
</Info>\
\
## Client event types\
\
<AccordionGroup>\
  <Accordion title="conversation_initiation_metadata">\
    - Automatically sent when starting a conversation\
    - Initializes conversation settings and parameters\
\
    ```javascript\
    // Example initialization metadata\
    \{\
      "type": "conversation_initiation_metadata",\
      "conversation_initiation_metadata_event": \{\
        "conversation_id": "conv_123",\
        "agent_output_audio_format": "pcm_44100",  // TTS output format\
        "user_input_audio_format": "pcm_16000"    // ASR input format\
      \}\
    \}\
    ```\
\
  </Accordion>\
\
  <Accordion title="ping">\
    - Health check event requiring immediate response\
    - Automatically handled by SDK\
    - Used to maintain WebSocket connection\
\
      ```javascript\
      // Example ping event structure\
      \{\
        "ping_event": \{\
          "event_id": 123456,\
          "ping_ms": 50  // Optional, estimated latency in milliseconds\
        \},\
        "type": "ping"\
      \}\
      ```\
\
      ```javascript\
      // Example ping handler\
      websocket.on('ping', () => \{\
        websocket.send('pong');\
      \});\
      ```\
\
  </Accordion>\
\
  <Accordion title="audio">\
    - Contains base64 encoded audio for playback\
    - Includes numeric event ID for tracking and sequencing\
    - Handles voice output streaming\
    \
    ```javascript\
    // Example audio event structure\
    \{\
      "audio_event": \{\
        "audio_base_64": "base64_encoded_audio_string",\
        "event_id": 12345\
      \},\
      "type": "audio"\
    \}\
    ```\
\
    ```javascript\
    // Example audio event handler\
    websocket.on('audio', (event) => \{\
      const \{ audio_event \} = event;\
      const \{ audio_base_64, event_id \} = audio_event;\
      audioPlayer.play(audio_base_64);\
    \});\
    ```\
\
  </Accordion>\
\
  <Accordion title="user_transcript">\
    - Contains finalized speech-to-text results\
    - Represents complete user utterances\
    - Used for conversation history\
\
    ```javascript\
    // Example transcript event structure\
    \{\
      "type": "user_transcript",\
      "user_transcription_event": \{\
        "user_transcript": "Hello, how can you help me today?"\
      \}\
    \}\
    ```\
\
    ```javascript\
    // Example transcript handler\
    websocket.on('user_transcript', (event) => \{\
      const \{ user_transcription_event \} = event;\
      const \{ user_transcript \} = user_transcription_event;\
      updateConversationHistory(user_transcript);\
    \});\
    ```\
\
  </Accordion>\
\
  <Accordion title="agent_response">\
    - Contains complete agent message\
    - Sent with first audio chunk\
    - Used for display and history\
\
    ```javascript\
    // Example response event structure\
    \{\
      "type": "agent_response",\
      "agent_response_event": \{\
        "agent_response": "Hello, how can I assist you today?"\
      \}\
    \}\
    ```\
\
    ```javascript\
    // Example response handler\
    websocket.on('agent_response', (event) => \{\
      const \{ agent_response_event \} = event;\
      const \{ agent_response \} = agent_response_event;\
      displayAgentMessage(agent_response);\
    \});\
    ```\
\
  </Accordion>\
\
  <Accordion title="agent_response_correction">\
    - Contains truncated response after interruption\
      - Updates displayed message\
      - Maintains conversation accuracy\
\
    ```javascript\
    // Example response correction event structure\
    \{\
      "type": "agent_response_correction",\
      "agent_response_correction_event": \{\
        "original_agent_response": "Let me tell you about the complete history...",\
        "corrected_agent_response": "Let me tell you about..."  // Truncated after interruption\
      \}\
    \}\
    ```\
\
    ```javascript\
    // Example response correction handler\
    websocket.on('agent_response_correction', (event) => \{\
      const \{ agent_response_correction_event \} = event;\
      const \{ corrected_agent_response \} = agent_response_correction_event;\
      displayAgentMessage(corrected_agent_response);\
    \});\
    ```\
\
  </Accordion>\
\
  <Accordion title="client_tool_call">\
    - Represents a function call the agent wants the client to execute\
    - Contains tool name, tool call ID, and parameters\
    - Requires client-side execution of the function and sending the result back to the server\
\
    <Info>\
      If you are using the SDK, callbacks are provided to handle sending the result back to the server.\
    </Info>\
\
    ```javascript\
    // Example tool call event structure\
    \{\
      "type": "client_tool_call",\
      "client_tool_call": \{\
        "tool_name": "search_database",\
        "tool_call_id": "call_123456",\
        "parameters": \{\
          "query": "user information",\
          "filters": \{\
            "date": "2024-01-01"\
          \}\
        \}\
      \}\
    \}\
    ```\
\
    ```javascript\
    // Example tool call handler\
    websocket.on('client_tool_call', async (event) => \{\
      const \{ client_tool_call \} = event;\
      const \{ tool_name, tool_call_id, parameters \} = client_tool_call;\
\
      try \{\
        const result = await executeClientTool(tool_name, parameters);\
        // Send success response back to continue conversation\
        websocket.send(\{\
          type: "client_tool_result",\
          tool_call_id: tool_call_id,\
          result: result,\
          is_error: false\
        \});\
      \} catch (error) \{\
        // Send error response if tool execution fails\
        websocket.send(\{\
          type: "client_tool_result",\
          tool_call_id: tool_call_id,\
          result: error.message,\
          is_error: true\
        \});\
      \}\
    \});\
    ```\
\
  </Accordion>\
  <Accordion title="vad_score">\
    - Voice Activity Detection score event\
    - Indicates the probability that the user is speaking\
    - Values range from 0 to 1, where higher values indicate higher confidence of speech\
\
    ```javascript\
    // Example VAD score event\
    \{\
      "type": "vad_score",\
      "vad_score_event": \{\
        "vad_score": 0.95\
      \}\
    \}\
    ```\
\
  </Accordion>\
</AccordionGroup>\
\
## Event flow\
\
Here's a typical sequence of events during a conversation:\
\
```mermaid\
sequenceDiagram\
    participant Client\
    participant Server\
\
    Server->>Client: conversation_initiation_metadata\
    Note over Client,Server: Connection established\
    Server->>Client: ping\
    Client->>Server: pong\
    Server->>Client: audio\
    Note over Client: Playing audio\
    Note over Client: User responds\
    Server->>Client: user_transcript\
    Server->>Client: agent_response\
    Server->>Client: audio\
    Server->>Client: client_tool_call\
    Note over Client: Client tool runs\
    Client->>Server: client_tool_result\
    Server->>Client: agent_response\
    Server->>Client: audio\
    Note over Client: Playing audio\
    Note over Client: Interruption detected\
    Server->>Client: agent_response_correction\
\
```\
\
### Best practices\
\
1. **Error handling**\
\
   - Implement proper error handling for each event type\
   - Log important events for debugging\
   - Handle connection interruptions gracefully\
\
2. **Audio management**\
\
   - Buffer audio chunks appropriately\
   - Implement proper cleanup on interruption\
   - Handle audio resource management\
\
3. **Connection management**\
\
   - Respond to PING events promptly\
   - Implement reconnection logic\
   - Monitor connection health\
\
## Troubleshooting\
\
<AccordionGroup>\
  <Accordion title="Connection issues">\
\
    - Ensure proper WebSocket connection\
    - Check PING/PONG responses\
    - Verify API credentials\
\
  </Accordion>\
  <Accordion title="Audio problems">\
\
    - Check audio chunk handling\
    - Verify audio format compatibility\
    - Monitor memory usage\
\
  </Accordion>\
  <Accordion title="Event handling">\
    - Log all events for debugging\
    - Implement error boundaries\
    - Check event handler registration\
  </Accordion>\
</AccordionGroup>\
\
<Info>\
  For detailed implementation examples, check our [SDK\
  documentation](/docs/conversational-ai/libraries/python).\
</Info>\
---\
title: Client to server events\
subtitle: >-\
  Send contextual information from the client to enhance conversational\
  applications in real-time.\
---\
\
**Client-to-server events** are messages that your application proactively sends to the server to provide additional context during conversations. These events enable you to enhance the conversation with relevant information without interrupting the conversational flow.\
\
<Note>\
  For information on events the server sends to the client, see the [Client\
  events](/docs/conversational-ai/customization/events/client-events) documentation.\
</Note>\
\
## Overview\
\
Your application can send contextual information to the server to improve conversation quality and relevance at any point during the conversation. This does not have to be in response to a client event received from the server. This is particularly useful for sharing UI state, user actions, or other environmental data that may not be directly communicated through voice.\
\
<Info>\
  While our SDKs provide helper methods for sending these events, understanding the underlying\
  protocol is valuable for custom implementations and advanced use cases.\
</Info>\
\
## Contextual updates\
\
The primary client-to-server event is the contextual update, which allows your application to send non-interrupting background information to the conversation.\
\
**Key characteristics:**\
\
- Updates are incorporated as background information in the conversation.\
- Does not interrupt the current conversation flow.\
- Useful for sending UI state, user actions, or environmental data.\
\
```javascript\
// Contextual update event structure\
\{\
  "type": "contextual_update",\
  "text": "User appears to be looking at pricing page"\
\}\
```\
\
```javascript\
// Example sending contextual updates\
function sendContextUpdate(information) \{\
  websocket.send(\
    JSON.stringify(\{\
      type: 'contextual_update',\
      text: information,\
    \})\
  );\
\}\
\
// Usage examples\
sendContextUpdate('Customer status: Premium tier');\
sendContextUpdate('User navigated to Help section');\
sendContextUpdate('Shopping cart contains 3 items');\
```\
\
## Best practices\
\
1. **Contextual updates**\
\
   - Send relevant but concise contextual information.\
   - Avoid overwhelming the LLM with too many updates.\
   - Focus on information that impacts the conversation flow or is important context from activity in a UI not accessible to the voice agent.\
\
2. **Timing considerations**\
\
   - Send updates at appropriate moments.\
   - Consider grouping multiple contextual updates into a single update (instead of sending every small change separately).\
\
<Info>\
  For detailed implementation examples, check our [SDK\
  documentation](/docs/conversational-ai/libraries/python).\
</Info>\
---\
title: Conversation flow\
subtitle: >-\
  Configure how your assistant handles timeouts and interruptions during\
  conversations.\
---\
\
## Overview\
\
Conversation flow settings determine how your assistant handles periods of user silence and interruptions during speech. These settings help create more natural conversations and can be customized based on your use case.\
\
<CardGroup cols=\{2\}>\
  <Card title="Timeouts" icon="clock" href="#timeouts">\
    Configure how long your assistant waits during periods of silence\
  </Card>\
  <Card title="Interruptions" icon="hand" href="#interruptions">\
    Control whether users can interrupt your assistant while speaking\
  </Card>\
</CardGroup>\
\
## Timeouts\
\
Timeout handling determines how long your assistant will wait during periods of user silence before prompting for a response.\
\
### Configuration\
\
Timeout settings can be configured in the agent's **Advanced** tab under **Turn Timeout**.\
\
The timeout duration is specified in seconds and determines how long the assistant will wait in silence before prompting the user. Turn timeouts must be between 1 and 30 seconds.\
\
#### Example Timeout Settings\
\
<Frame background="subtle">\
  ![Timeout settings](file:bc2d0c44-21cf-424c-ab7b-f046804e0bde)\
</Frame>\
\
<Note>\
  Choose an appropriate timeout duration based on your use case. Shorter timeouts create more\
  responsive conversations but may interrupt users who need more time to respond, leading to a less\
  natural conversation.\
</Note>\
\
### Best practices for timeouts\
\
- Set shorter timeouts (5-10 seconds) for casual conversations where quick back-and-forth is expected\
- Use longer timeouts (10-30 seconds) when users may need more time to think or formulate complex responses\
- Consider your user context - customer service may benefit from shorter timeouts while technical support may need longer ones\
\
## Interruptions\
\
Interruption handling determines whether users can interrupt your assistant while it's speaking.\
\
### Configuration\
\
Interruption settings can be configured in the agent's **Advanced** tab under **Client Events**.\
\
To enable interruptions, make sure interruption is a selected client event.\
\
#### Interruptions Enabled\
\
<Frame background="subtle">\
  ![Interruption allowed](file:96e2987c-e3f3-4d67-8c16-0e6d22604313)\
</Frame>\
\
#### Interruptions Disabled\
\
<Frame background="subtle">\
  ![Interruption ignored](file:8dc6de95-8465-460c-ad73-7161e5078161)\
</Frame>\
\
<Note>\
  Disable interruptions when the complete delivery of information is crucial, such as legal\
  disclaimers or safety instructions.\
</Note>\
\
### Best practices for interruptions\
\
- Enable interruptions for natural conversational flows where back-and-forth dialogue is expected\
- Disable interruptions when message completion is critical (e.g., terms and conditions, safety information)\
- Consider your use case context - customer service may benefit from interruptions while information delivery may not\
\
## Recommended configurations\
\
<AccordionGroup>\
  <Accordion title="Customer service">\
    - Shorter timeouts (5-10 seconds) for responsive interactions - Enable interruptions to allow\
    customers to interject with questions\
  </Accordion>\
  <Accordion title="Legal disclaimers">\
    - Longer timeouts (15-30 seconds) to allow for complex responses - Disable interruptions to\
    ensure full delivery of legal information\
  </Accordion>\
  <Accordion title="Conversational EdTech">\
    - Longer timeouts (10-30 seconds) to allow time to think and formulate responses - Enable\
    interruptions to allow students to interject with questions\
  </Accordion>\
</AccordionGroup>\
---\
title: Authentication\
subtitle: Learn how to secure access to your conversational AI agents\
---\
\
## Overview\
\
When building conversational AI agents, you may need to restrict access to certain agents or conversations. ElevenLabs provides multiple authentication mechanisms to ensure only authorized users can interact with your agents.\
\
## Authentication methods\
\
ElevenLabs offers two primary methods to secure your conversational AI agents:\
\
<CardGroup cols=\{2\}>\
  <Card title="Signed URLs" icon="signature" href="#using-signed-urls">\
    Generate temporary authenticated URLs for secure client-side connections without exposing API\
    keys.\
  </Card>\
  <Card title="Allowlists" icon="list-check" href="#using-allowlists">\
    Restrict access to specific domains or hostnames that can connect to your agent.\
  </Card>\
</CardGroup>\
\
## Using signed URLs\
\
Signed URLs are the recommended approach for client-side applications. This method allows you to authenticate users without exposing your API key.\
\
<Note>\
  The guides below uses the [JS client](https://www.npmjs.com/package/@11labs/client) and [Python\
  SDK](https://github.com/elevenlabs/elevenlabs-python/).\
</Note>\
\
### How signed URLs work\
\
1. Your server requests a signed URL from ElevenLabs using your API key.\
2. ElevenLabs generates a temporary token and returns a signed WebSocket URL.\
3. Your client application uses this signed URL to establish a WebSocket connection.\
4. The signed URL expires after 15 minutes.\
\
<Warning>Never expose your ElevenLabs API key client-side.</Warning>\
\
### Generate a signed URL via the API\
\
To obtain a signed URL, make a request to the `get_signed_url` [endpoint](/docs/conversational-ai/api-reference/conversations/get-signed-url) with your agent ID:\
\
<CodeBlocks>\
```python\
# Server-side code using the Python SDK\
from elevenlabs.client import ElevenLabs\
async def get_signed_url():\
    try:\
        elevenlabs = ElevenLabs(api_key="your-api-key")\
        response = await elevenlabs.conversational_ai.conversations.get_signed_url(agent_id="your-agent-id")\
        return response.signed_url\
    except Exception as error:\
        print(f"Error getting signed URL: \{error\}")\
        raise\
```\
\
```javascript\
import \{ ElevenLabsClient \} from 'elevenlabs';\
\
// Server-side code using the JavaScript SDK\
const elevenlabs = new ElevenLabsClient(\{ apiKey: 'your-api-key' \});\
async function getSignedUrl() \{\
  try \{\
    const response = await elevenlabs.conversationalAi.conversations.getSignedUrl(\{\
      agent_id: 'your-agent-id',\
    \});\
\
    return response.signed_url;\
  \} catch (error) \{\
    console.error('Error getting signed URL:', error);\
    throw error;\
  \}\
\}\
```\
\
```bash\
curl -X GET "https://api.elevenlabs.io/v1/convai/conversation/get-signed-url?agent_id=your-agent-id" \\\
-H "xi-api-key: your-api-key"\
```\
\
</CodeBlocks>\
\
The curl response has the following format:\
\
```json\
\{\
  "signed_url": "wss://api.elevenlabs.io/v1/convai/conversation?agent_id=your-agent-id&conversation_signature=your-token"\
\}\
```\
\
### Connecting to your agent using a signed URL\
\
Retrieve the server generated signed URL from the client and use the signed URL to connect to the websocket.\
\
<CodeBlocks>\
\
```python\
# Client-side code using the Python SDK\
from elevenlabs.conversational_ai.conversation import (\
    Conversation,\
    AudioInterface,\
    ClientTools,\
    ConversationInitiationData\
)\
import os\
from elevenlabs.client import ElevenLabs\
api_key = os.getenv("ELEVENLABS_API_KEY")\
\
elevenlabs = ElevenLabs(api_key=api_key)\
\
conversation = Conversation(\
  client=elevenlabs,\
  agent_id=os.getenv("AGENT_ID"),\
  requires_auth=True,\
  audio_interface=AudioInterface(),\
  config=ConversationInitiationData()\
)\
\
async def start_conversation():\
  try:\
    signed_url = await get_signed_url()\
    conversation = Conversation(\
      client=elevenlabs,\
      url=signed_url,\
    )\
\
    conversation.start_session()\
  except Exception as error:\
    print(f"Failed to start conversation: \{error\}")\
\
```\
\
```javascript\
// Client-side code using the JavaScript SDK\
import \{ Conversation \} from '@11labs/client';\
\
async function startConversation() \{\
  try \{\
    const signedUrl = await getSignedUrl();\
    const conversation = await Conversation.startSession(\{\
      signedUrl,\
    \});\
\
    return conversation;\
  \} catch (error) \{\
    console.error('Failed to start conversation:', error);\
    throw error;\
  \}\
\}\
```\
\
</CodeBlocks>\
\
### Signed URL expiration\
\
Signed URLs are valid for 15 minutes. The conversation session can last longer, but the conversation must be initiated within the 15 minute window.\
\
## Using allowlists\
\
Allowlists provide a way to restrict access to your conversational AI agents based on the origin domain. This ensures that only requests from approved domains can connect to your agent.\
\
### How allowlists work\
\
1. You configure a list of approved hostnames for your agent.\
2. When a client attempts to connect, ElevenLabs checks if the request's origin matches an allowed hostname.\
3. If the origin is on the allowlist, the connection is permitted; otherwise, it's rejected.\
\
### Configuring allowlists\
\
Allowlists are configured as part of your agent's authentication settings. You can specify up to 10 unique hostnames that are allowed to connect to your agent.\
\
### Example: setting up an allowlist\
\
<CodeBlocks>\
\
```python\
from elevenlabs.client import ElevenLabs\
import os\
from elevenlabs.types import *\
\
api_key = os.getenv("ELEVENLABS_API_KEY")\
elevenlabs = ElevenLabs(api_key=api_key)\
\
agent = elevenlabs.conversational_ai.agents.create(\
  conversation_config=ConversationalConfig(\
    agent=AgentConfig(\
      first_message="Hi. I'm an authenticated agent.",\
    )\
  ),\
  platform_settings=AgentPlatformSettingsRequestModel(\
  auth=AuthSettings(\
    enable_auth=False,\
    allowlist=[\
      AllowlistItem(hostname="example.com"),\
      AllowlistItem(hostname="app.example.com"),\
      AllowlistItem(hostname="localhost:3000")\
      ]\
    )\
  )\
)\
```\
\
```javascript\
async function createAuthenticatedAgent(client) \{\
  try \{\
    const agent = await elevenlabs.conversationalAi.agents.create(\{\
      conversation_config: \{\
        agent: \{\
          first_message: "Hi. I'm an authenticated agent.",\
        \},\
      \},\
      platform_settings: \{\
        auth: \{\
          enable_auth: false,\
          allowlist: [\
            \{ hostname: 'example.com' \},\
            \{ hostname: 'app.example.com' \},\
            \{ hostname: 'localhost:3000' \},\
          ],\
        \},\
      \},\
    \});\
\
    return agent;\
  \} catch (error) \{\
    console.error('Error creating agent:', error);\
    throw error;\
  \}\
\}\
```\
\
</CodeBlocks>\
\
## Combining authentication methods\
\
For maximum security, you can combine both authentication methods:\
\
1. Use `enable_auth` to require signed URLs.\
2. Configure an allowlist to restrict which domains can request those signed URLs.\
\
This creates a two-layer authentication system where clients must:\
\
- Connect from an approved domain\
- Possess a valid signed URL\
\
<CodeBlocks>\
\
```python\
from elevenlabs.client import ElevenLabs\
import os\
from elevenlabs.types import *\
api_key = os.getenv("ELEVENLABS_API_KEY")\
elevenlabs = ElevenLabs(api_key=api_key)\
agent = elevenlabs.conversational_ai.agents.create(\
  conversation_config=ConversationalConfig(\
    agent=AgentConfig(\
      first_message="Hi. I'm an authenticated agent that can only be called from certain domains.",\
    )\
  ),\
platform_settings=AgentPlatformSettingsRequestModel(\
  auth=AuthSettings(\
    enable_auth=True,\
    allowlist=[\
      AllowlistItem(hostname="example.com"),\
      AllowlistItem(hostname="app.example.com"),\
      AllowlistItem(hostname="localhost:3000")\
    ]\
  )\
)\
```\
\
```javascript\
async function createAuthenticatedAgent(elevenlabs) \{\
  try \{\
    const agent = await elevenlabs.conversationalAi.agents.create(\{\
      conversation_config: \{\
        agent: \{\
          first_message: "Hi. I'm an authenticated agent.",\
        \},\
      \},\
      platform_settings: \{\
        auth: \{\
          enable_auth: true,\
          allowlist: [\
            \{ hostname: 'example.com' \},\
            \{ hostname: 'app.example.com' \},\
            \{ hostname: 'localhost:3000' \},\
          ],\
        \},\
      \},\
    \});\
\
    return agent;\
  \} catch (error) \{\
    console.error('Error creating agent:', error);\
    throw error;\
  \}\
\}\
```\
\
</CodeBlocks>\
\
## FAQ\
\
<AccordionGroup>\
  <Accordion title="Can I use the same signed URL for multiple users?">\
    This is possible but we recommend generating a new signed URL for each user session.\
  </Accordion>\
  <Accordion title="What happens if the signed URL expires during a conversation?">\
    If the signed URL expires (after 15 minutes), any WebSocket connection created with that signed\
    url will **not** be closed, but trying to create a new connection with that signed URL will\
    fail.\
  </Accordion>\
  <Accordion title="Can I restrict access to specific users?">\
    The signed URL mechanism only verifies that the request came from an authorized source. To\
    restrict access to specific users, implement user authentication in your application before\
    requesting the signed URL.\
  </Accordion>\
  <Accordion title="Is there a limit to how many signed URLs I can generate?">\
    There is no specific limit on the number of signed URLs you can generate.\
  </Accordion>\
  <Accordion title="How do allowlists handle subdomains?">\
    Allowlists perform exact matching on hostnames. If you want to allow both a domain and its\
    subdomains, you need to add each one separately (e.g., "example.com" and "app.example.com").\
  </Accordion>\
  <Accordion title="Do I need to use both authentication methods?">\
    No, you can use either signed URLs or allowlists independently based on your security\
    requirements. For highest security, we recommend using both.\
  </Accordion>\
  <Accordion title="What other security measures should I implement?">\
    Beyond signed URLs and allowlists, consider implementing:\
\
    - User authentication before requesting signed URLs\
    - Rate limiting on API requests\
    - Usage monitoring for suspicious patterns\
    - Proper error handling for auth failures\
\
  </Accordion>\
</AccordionGroup>\
---\
title: SIP trunking\
subtitle: >-\
  Connect your existing phone system with ElevenLabs conversational AI agents\
  using SIP trunking\
---}